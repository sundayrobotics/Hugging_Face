{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long short term memory\n",
    "# a special recurrent neural net\n",
    "# brings persistence in inferencing. we do not start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization allows one make a trade-off btn performance and accuracy with a known models after\n",
    "# its training is complete\n",
    "\n",
    "# Quantization -> instantiate a floating point model and create a quantized version of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the modules to be used in the recpe\n",
    "\n",
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_for_demo(nn.Module):\n",
    "    \"\"\"\n",
    "        Elementary long short term memory style.\n",
    "        Wraps up an nn.LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, depth):\n",
    "        super(lstm_for_demo, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, out_dim, depth)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        out, hidden = self.lstm(inputs, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "torch.manual_seed(29592) #seeds for reproducibility\n",
    "\n",
    "#shape params\n",
    "model_dimension=800\n",
    "sequence_length = 20\n",
    "batch_size = 1\n",
    "lstm_depth = 1\n",
    "\n",
    "#random data for input\n",
    "inputs = torch.randn(sequence_length, batch_size, model_dimension)\n",
    "#hidden is actually a tuple of the inintial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth, batch_size, model_dimension), \n",
    "torch.randn(lstm_depth, batch_size, model_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floating-point version of the module lstm_for_demo(\n",
      "  (lstm): LSTM(800, 800)\n",
      ")\n",
      "quantized version lstm_for_demo(\n",
      "  (lstm): DynamicQuantizedLSTM(800, 800)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a floating point instance\n",
    "float_lstm = lstm_for_demo(model_dimension, model_dimension, lstm_depth)\n",
    "\n",
    "# quantizing -> returns a quantized version of the module\n",
    "quantized_lstm = torch.quantization.quantize_dynamic(\n",
    "    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(f\"floating-point version of the module {float_lstm}\")\n",
    "print(f'quantized version {quantized_lstm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: fp32 \t size(kb): 20507.039\n",
      "model: int8 \t size(kb): 5147.551\n",
      "3.98 times smaller\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\" \"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(f\"model: {label} \\t size(kb): {size/1e3}\" )\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "#comparing the sizes\n",
    "f = print_size_of_model(float_lstm, \"fp32\")\n",
    "q = print_size_of_model(quantized_lstm, \"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs value of output tensor values in fp32 model is 0.11407\n",
      "Mean abs value of output tensor values in int8 model is 0.11412\n"
     ]
    }
   ],
   "source": [
    "# run the float modesl\n",
    "out1, hidden1 = float_lstm(inputs, hidden)\n",
    "mag1 = torch.mean(abs(out1)).item()\n",
    "print(\"Mean abs value of output tensor values in fp32 model is {0:.5f}\".format(mag1))\n",
    "\n",
    "# run the quant model\n",
    "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
    "mag2 = torch.mean(abs(out2)).item()\n",
    "print(\"Mean abs value of output tensor values in int8 model is {0:.5f}\".format(mag2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latency: tend to run faster -> int8 ops/ less time moving param data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating Point\n",
      "18 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Quant INT8\n",
      "2.21 ms ± 231 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# compare the performance\n",
    "print(\"Floating Point\")\n",
    "%timeit float_lstm.forward(inputs,hidden)\n",
    "\n",
    "print(\"Quant INT8\")\n",
    "%timeit quantized_lstm.forward(inputs,hidden)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc128987b80414afb03073ef0ac1a5f16932121e6908c8d5ab86fd86be48a1ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
